---
author:
  - name: David L Miller
    affiliation: University of St Andrews
    address: >
      Centre for Research into Ecological and Environmental Modelling,
      The Observatory,
      St Andrews, Fife KY16 9LZ, Scotland
    email: dave@ninepointeightone.net
    url: http://converged.yt
  - name: Eric Rexstad
    affiliation: University of St Andrews
    address: >
      Centre for Research into Ecological and Environmental Modelling,
      The Observatory,
      St Andrews, Fife KY16 9LZ, Scotland
    email: Eric.Rexstad@st-andrews.ac.uk
  - name: Len Thomas
    affiliation: University of St Andrews
    address: >
      Centre for Research into Ecological and Environmental Modelling,
      The Observatory,
      St Andrews, Fife KY16 9LZ, Scotland
    email: len.thomas@st-andrews.ac.uk
  - name: Laura Marshall
    affiliation: University of St Andrews
    address: >
      Centre for Research into Ecological and Environmental Modelling,
      The Observatory,
      St Andrews, Fife KY16 9LZ, Scotland
    email: lhm@st-andrews.ac.uk
  - name: Jeffrey L Laake
    affiliation: NOAA
    address: >
      National Marine Mammal Laboratory
      Alaska Fisheries Science Center
      7600 Sand Point Way N.E., Seattle, WA 98115, USA
    email: Jeff.Laake@noaa.gov
title:
  formatted: "Distance Sampling in \\proglang{R}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Distance Sampling in R"
  # For running headers, if needed
  #short:     "\\pkg{foo}: A Capitalized Title"
abstract: >
 
  The study of plants or animals often involves the estimation of population size and/or their spatial distribution. These characteristics are important considerations in wildlife management. We introduce the \proglang{R} package \pkg{Distance} that uses distance sampling data to estimate populations size (abundance) and is compatible with another \proglang{R} package \pkg{dsm} which can be used to model spatial distribution. We describe how users can obtain estimates of abundance and/or density using the package \pkg{Distance} as well documenting the links it provides with other \proglang{R} packages offering specialist solutions to specific distance sampling challenges. We demonstrate how this package provides a migration pathway from previous software, thereby allowing us to deliver cutting edge methods to the users more quickly. 
  
keywords:
  # at least one keyword must be supplied
  formatted: [distance sampling, abundance estimation, line transects, point transects, detection functions, Horvitz-Thompson, "\\proglang{R}", Distance]
  plain:     [distance sampling, abundance estimation, line transects, point transects, detection functions, Horvitz-Thompson, R, Distance]
preamble: >
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{longtable}
  \usepackage{booktabs}
  \usepackage[colorinlistoftodos, color=green!40, textwidth=60, textsize=small]{todonotes}

output: rticles::jss_article
bibliography: jstatsoft.bib
---

# Introduction

```{r setup, include=FALSE}
library("knitr")
opts_chunk$set(cache=TRUE)
```

<!--
(LT: I wonder who the intended audience is for this paper? (LM: - Distance for Windows users and other people looking do distance sampling in R?) The way it is currently written, it ``kinda'' introduces distance sampling as well as the relevant commands.  Is this standard for a paper in this journal -- to introduce the methods as well as the software?  Can we assume a reasonable level of analytical sophistication on the part of the reader, and so start with a few more formulae?  Can we assume they know the methods, more or less, or point them at somewher where this is already introduced.  (Perhaps I should go back and have a look at the EcolApps paper to see what approach we took there... (LM: The paper on Distance in JAppEcol? There is quite a big intro on distance sampling there.))) -->

Distance sampling [@Buckland:2001vm; @Buckland:2004ts; @buckland2015distance] encompasses a suite of methods used to estimate the density and/or abundance of biological populations. Distance sampling can be thought of as an extension of plot sampling. Plot sampling involves selecting a number of plots (small areas) at random within the study area and counting the objects of interest that are contained within each plot. By selecting the plots at random we can assume that the density of objects in the plots is representative of the study area as a whole. One of the key assumptions of plot sampling is that all objects within each of the plots are counted. Distance sampling relaxes this assumption in that observers are no longer required to see and count everything within selected plots. While plot sampling techniques are adequate for static populations occuring at high density they are inefficient for more sparsely distributed populations. Distance sampling provides a more efficient solution in such circumstances.

Conventional distance sampling assumes the observer is either stood at a point or moving along a line and will only see everything that occurs exactly at that point or on the line. It is then expected that the further away an object is from the point or line (also known as the sampler or transect) the less likely it is that the observer will see it. By recording the distance to each of the detected objects we can fit a model to describe the probability of detection given distance from the sampler, which we refer to as the ‘detection function’. The detection function can be used to infer how many objects were missed and thereby produce estimates of density and/or abundance. Further information on distance sampling, including the theory, various extensions to deal with more complex situations and recent methodological developments can be found at http://distancesampling.org/dbib.html

The Windows program Distance (or "DISTANCE"; for clarity henceforth "Distance for Windows") can be used to fit detection functions to such distance sampling data. It was first released in November 1998 as Distance for Windows version 3.5. Since this time it has evolved to include various design and analysis features [@Thomas:2010cf]. Distance for Windows versions 5 onwards have included \proglang{R} [@rcore]  packages as the analysis engines providing additional, more complex analysis options than those offered by the original FORTRAN engine. 

As Distance for Windows becomes increasingly reliant on analyses performed in \proglang{R} and new methods are being developed at a rate where it is unfeasible to make all these available in the Windows interface, we are encouraging the use of our \proglang{R} packages directly. In addition, the core packages in \proglang{R} provide a variety of functions useful for data exploration with minimal data formatting. The plotting of a histogram of detection distances only requires these values to be stored in a numerical vector, an exercise that is recommended not only prior to analyses but also early on in the data collection process _(LM need ref here @buckland2015distance ?)_. 

Until now those wishing to use our \proglang{R} packages for straight forward distance sampling analyses would have had to negotiate the complex package \pkg{mrds} [@mrds-pkg] designed for mark-recapture distance sampling. In addition, \pkg{mrds} requires a complex data structure before analyses can be completed. \pkg{Distance} is a wrapper package around \pkg{mrds} making it easier to get started with basic distance sampling analyses in \proglang{R}. Like the histogram function, the most basic detection function estimation only requires a numeric vector of distances. Here we demonstrate how to use \pkg{Distance} to fit detection functions, perform model checking and selection, and estimate abundance and/or density.



## Distance sampling in R? 

_(LM I wonder if here we should focus on comparing Distance to other R packages offering distance sampling analyses here? Some of the info below could be incorporated into the paragraphs above (I have done this to some extent.. do we need more on distance sampling theory above?)... not done much with this section in case we want to do something different here.)_

<!-- While plot sampling techniques are adequate for static populations occuring at high density they are inefficient for more sparsely distributed populations. Distance sampling provides a more efficient solution in such circumstances. In addition, for cryptic species it simply may not be possible to detect all individuals within a given plot and we must find a way to account for imperfect detectability when estimating abundance [@LahozMonfort:2013bg]. This paper does not provide a detailed explanation of the theory behind distance sampling but instead concentrates on it's application using the package \pkg{Distance}. Further information on distance sampling, including the theory, various extensions to deal with more complex situations and recent methodological developments can be found at http://distancesampling.org/dbib.html -->

Conventional distance sampling analysis simply relies on estimating the probability of detection given how far an object is from the sampler. Note that in the case of populations that occur in groups (clusters) the whole group or cluster is treated as an individual object. Figure \ref{fig:lines} shows example data collection on a simulated population of 500 individuals using line transects and the corresponding histogram of distances recorded by the observer. As expected we see in the histogram of Fig. \ref{fig:lines} that as the perpendicular distance from the transect line increases, the number of detections made in each distance interval decreases. \pkg{Distance} provides a selection of functions to model this probability of detection detailed in Section... below.

```{r lines, fig.height=3, fig.width=7, echo=FALSE, message=FALSE, fig.cap="Left side plot shows an example of a survey of an area containing a population of 500 objects; blue lines indicate sampler placement, red dots indicate detected individuals and grey dots give the locations of unobserved individuals. The right side of the figure shows a histogram of observed distances.\\label{fig:lines}"}
opar <- par(mfrow=c(1,2))
# line and point transect examples
# top row points/samplers in space
# bottom row histograms

# generate some animals to sample
set.seed(131) # same results every time
library("mgcv") # for inSide

# generate population locations
N <- 500
x <- runif(N)
y <- runif(N)

## line transects
opar2 <- par(mar=c(1, 1, 1, 1) + 0.1)
plot(x, y, pch=19, asp=1, cex=0.6, main="",col="grey", axes=FALSE, xlab="", ylab="")
polygon(x=c(0,0,1,1,0), y=c(0,1,1,0,0))
# generate some lines
# in this case we don't randomise the offset of the grid
lt <- list(x=c(-0.0125,-0.0125,0.0125,0.0125,-0.0125), y=c(0,1,1,0,0))
# set sigma
sigma <- 0.01
# storage for detected distances
detected_distances <- c()
for(i in 1:5){
  # calculate next strip location
  lt$x <- lt$x+0.15
  # plot the line transect
  lines(x=rep(mean(range(lt$x)),2), y=c(0,1), col="blue",lty=2)
  # calculate the distances to objects from the line
  distances <- abs(lt$x - x)
  # randomly decide which were detected
  detected <- exp(-distances^2/(2*sigma^2)) > runif(length(distances))
  # plot those objects detected
  points(x[detected], y[detected], pch=19, cex=0.6, col="red")
  # collect the distances to detected objects
  detected_distances <- c(detected_distances, distances[detected])
}

#par(opar2)
par(mar=c(4, 4, 1, 2) + 0.1)
hist(detected_distances, main="", xlab="Observed perpendicular distances", freq = FALSE)

par(opar)

```


The probability of detecting an object may not only depend on how far it is from the obsever but also on other factors such as weather conditions, ground cover, cluster size etc. The \pkg{Distance} package also allows the incorporation of such covariates into the modelling process of the detection function.

_(LM How does Distance use the detection function to estimate abundance is it any different to mrds does it infact use dht?)_ 

<!--

(LM: do we need to talk about assumptions here?)

(LM: detail what Distance deals with?)

Field methods are chosen to be suitable to species and habitat constraints [@buckland2015distance].

```{r points-and-lines, fig.height=6, fig.width=7, echo=FALSE, message=FALSE, fig.cap="Left side plots show an example of a survey of an area containing a population of 500 objects; blue lines (top plot) and triangles (bottom plot) indicate sampler placement, red dots indicate detected individuals and grey dots give the locations of unobserved individuals. The right side of the figure shows histograms of observed distances (again, lines top and points bottom).\\label{fig:pointslines}"}
opar <- par(mfrow=c(2,2))
# line and point transect examples
# top row points/samplers in space
# bottom row histograms

# generate some animals to sample
set.seed(131) # same results every time
library("mgcv") # for inSide

# generate population locations
N <- 500
x <- runif(N)
y <- runif(N)

## line transects
opar2 <- par(mar=c(1, 1, 1, 1) + 0.1)
plot(x, y, pch=19, asp=1, cex=0.6, main="",col="grey", axes=FALSE, xlab="", ylab="")
polygon(x=c(0,0,1,1,0), y=c(0,1,1,0,0))
# generate some lines
# in this case we don't randomise the offset of the grid
lt <- list(x=c(-0.0125,-0.0125,0.0125,0.0125,-0.0125), y=c(0,1,1,0,0))
# set sigma
sigma <- 0.01
# storage for detected distances
detected_distances <- c()
for(i in 1:5){
  # calculate next strip location
  lt$x <- lt$x+0.15
  # plot the line transect
  lines(x=rep(mean(range(lt$x)),2), y=c(0,1), col="blue",lty=2)
  # calculate the distances to objects from the line
  distances <- abs(lt$x - x)
  # randomly decide which were detected
  detected <- exp(-distances^2/(2*sigma^2)) > runif(length(distances))
  # plot those objects detected
  points(x[detected], y[detected], pch=19, cex=0.6, col="red")
  # collect the distances to detected objects
  detected_distances <- c(detected_distances, distances[detected])
}

#par(opar2)
par(mar=c(4, 4, 1, 2) + 0.1)
hist(detected_distances, main="", xlab="Observed perpendicular distances")

## point transects
par(mar=c(1, 1, 1, 1) + 0.1)
plot(x, y, pch=19, asp=1, cex=0.6, main="", col="grey", axes=FALSE, xlab="", ylab="")
polygon(x=c(0,0,1,1,0), y=c(0,1,1,0,0))

# set sigma
sigma <- 0.05
# storage for detected distances
detected_distances <- c()
# lay out a grid of points
pt <- as.list(expand.grid(x=seq(0.15, 0.85, len=3), y=seq(0.15, 0.85, len=3)))
for(i in 1:length(pt$x)){
  # generate point location
  # plot the point transect
##LT: Changed from pch19 to pch4 so that colour-blind people can see where the samples are
#  points(pt$x[i], pt$y[i], pch=19, col="blue",cex=0.7)
  points(pt$x[i], pt$y[i], pch=17, col="blue",cex=1.5)
  # calculate the distances to objects from the line
  distances <- sqrt((pt$x[i] - x)^2+(pt$y[i]-y)^2)
  # randomly decide which were detected
  detected <- exp(-distances^2/(2*sigma^2)) > runif(length(distances))
  # plot those objects detected
  points(x[detected], y[detected], pch=19, cex=0.6, col="red")
  # collect the distances to detected objects
  detected_distances <- c(detected_distances, distances[detected])
}

par(mar=c(4, 4, 1, 2) + 0.1)
hist(detected_distances, main="", xlab="Observed radial distances")

par(opar)

```

(LT: What is said about the decreasing number of observations is incorrect, really, and the "taking account of" just makes it confusing, I think.  Needs a bit more editing - possibly re-order to explain what we see first.) For both points and lines, given the geometry of the sampler (see "Detection functions"), the histogram of distances should show a decreasing number of observations with increasing distance from the sampler (having taken into account sample geometry, see below). For line transects we expect objects to be distributed uniformly with respect to distance from the line (assuming that lines and points are placed independently of animal distribution) with detectability causing the number of detections to decrease (as distance from the sampler increases). For point transects we recognise that as distance from the point increases, the area of the circle with a corresponding radius increases with distance squared, hence the number of objects available to be detected is a linearly increasing function of distance from the point.

(LT: A detection function is not a line; it is also scaled differently.  This needs editing.) Plotting a histogram of observed distances, we can crudely estimate the drop-off in detectability by eye if we trace a line that approximates the tops of the histogram bars: this is a detection function. \pkg{Distance} estimates the parameters for a fixed-form detection function using maximum likelihood estimation. We address possible models below.

(LT: stranded one-sentence paragraph -- needs integrating.) 

The rest of the paper has the following structure: we describe data organisation for use with \pkg{Distance}; models for the detection function are described in terms of formulation and examples of fitting in \proglang{R}. We then show how to perform model checking, goodness of fit and model selection. We go on to show how to estimate abundance, including stratified estimates of abundance. The final two sections of the article look at extensions (both in terms of methodology and software) and put the package in a broader context amongst other software packages used for estimating the abundance of biological populations. (LT: Do we mean software packages in general (e.g., Mark) or R packages?  Do we actually do this -- i.e., talk about how Distance fits in among all this, or is this now an appendix, and does it cover all methods or just distance-based methods.)

(LT: Only got this far: will do more later...) -->

# Data

We introduce two example analyses performed in \pkg{Distance}: one line transect and one point transect. These data sets have been chosen as they represent typical data seen in practice. 

### Minke whales

The line transect data have been simulated from models fitted to Antarctic minke whale (*Balaenoptera bonaerensis*) data (_LM why is the Minke whale data simulated rather than ust using original data, just data ownership/permissions?_). These data were collected as part of the International Whaling Commission's International Decade of Cetacean Research Southern Ocean Whale and Ecosystem Research (IWC IDCR-SOWER) programme 1992-1993 austral summer surveys [@Branch:2001ua]. This data set consists of 99 observations that are stratified based on location (near or distant from ice edge) and effort data (transect lengths)(LM did the original dataset have 99 observations or the simulated dataset or both?). The survey is shown in Figure \ref{fig:minke-strata}. _(LM wonder if we need more info on how the data were simulated... based only on detection function or on spatial model too? I notice in the figure text that the simulated data are based on ``1992/93 Area III'' should this info go here too?)_

\begin{figure}
\includegraphics{minke-strata}
\caption{Strata used for the minke whale data adapted from Hedley and Buckland (2004). Points show the (simulated) locations of observations along transect lines. The stepped line shows the boundary between North and South strata. Further details on the survey are available in Branch and Butterworth (2001) (simulated data is based on ``1992/93 Area III'' therein).\label{fig:minke-strata}}
\end{figure}

### Amakihi

The point transect data set consists of 1485 observations of Amakihi (*Hemignathus virens*; a Hawaiian songbird), collected at 41 points between 1992 and 1995. The data include distances and two covariates collected during the survey: observer (a three level factor), time after sunrise (transformed to minutes (continuous) or hours (factor) covariates). Data are analysed comprehensively in @Marques:2007ey.

## Data setup

The two example data sets used here are distributed preformatted with \pkg{Distance} so readers can reproduce our analyses. Generally, data collected in the field will require some formatting for use with \pkg{Distance}. However, the package allows for a flexible format of data input ranging from very simple to complex dependent on the type of analysis you wish to perform:

* In the simplest case, where the objective is to estimate a detection function, all that is needed is a vector of distances.
* To include additional covariates into the detection function (see "Detection functions") we use a \code{data.frame}. The \code{data.frame} contains a column called \code{distance} (containing the observed distances) and additional named columns for covariates potentially useful in modelling detectability (for example \code{observer} or \code{seastate}). Some column names are reserved: \code{object} for an observation identifier (see "Extensions"), \code{size} for group or cluster size (see "Detection functions" and "Abundance and variance estimation"), \code{detected} for whether an observation was detected (see "Extensions") and the columns described in the next bullet.
* To estimate abundance beyond the area covered by expended survey effort, additional information is required. This consists of information about transect and occasion the observation was made (the \code{Sample.Label}), a column named \code{Effort} which gives the effort associated with that sample (for lines their length and for points the number of times that point was visited), the stratum the sample was located in (the \code{Region.Label}, which may be from pre- or post-survey stratification, see "Estimating abundance and variance") and the stratum areas (\code{Area}). For transects which were surveyed but have no observations we enter \code{NA} in their \code{distance} column. We refer to this data format (where all information is contained in one table) as "flatfile" as it can be easily created in a single spreadsheet.

As we will see in "Extensions", further information is also required when using more complex models.

It is also possible to use distances collected in intervals ("binned" or "grouped" data, as opposed to "exact" distances) to model the detection function. In this case a set of cutpoints for the bins must be provided rather than the \code{distance} column, these are specified as two columns \code{distbegin} and \code{distend}. More information on binned data is included in @Buckland:2001vm sections 4.5 and 7.4.1.2.

The columns \code{distance}, \code{Area} and (in the case of line transects) \code{Effort} have associated units (though these are not explicitly included in a \pkg{Distance} analysis. For this reason, we recommended that these are converted to SI units before starting any analysis to ensure that resulting abundance and density estimates have sensible units.

The minke whale data follows the "flatfile" format given in the last bullet point:
```{r load-data, echo=FALSE}
# quietly load the data
load("data/minke.RData")
load("data/amakihi.RData")
```
```{r minke-data-head, message=FALSE}
library("Distance")
head(minke)
```
Whereas the amakihi data lacks effort and stratum data:
```{r amakihi-data-head}
head(amakihi)
```

We will explore the consequences of including effort and stratum data during analysis below.

# Detection functions

The detection function describes the relationship between observed distances and probability of detection. The detection function itself models the probability $\mathbb{P}(\text{object detected } \vert \text{ object at distance } y)$ and is usually denoted $g(y; \boldsymbol{\theta})$ where $y$ is distance (from a line or point) and $\boldsymbol{\theta}$ is a vector of parameters to be estimated. Our goal is to estimate an *average probability of detection* ($p$, average in the sense of an average over distance from $0$ to $w$), so we must integrate out distance ($y$) from the detection function:
$$
p = \int_0^w \pi(y) g(y; \boldsymbol{\theta}) dy
$$
where $\pi(y)$ describes the distribution of objects with respect to the sampler; $\pi(x)=1/w$ for line transects and $\pi(r)=\frac{2r}{w^2}$ for point transects, taking into account the geometry of the sampler [@Buckland:2001vm, Chapter 3]. When considering a particular transect type we let $x$ denote a perpendicular distance from a line and $r$ denote radial distance from a point (rather than using $y$).

It is crucial that the detection function accurately models detectability at small distances; we are less worried by its behaviour further away from 0. To ensure that the model is not overly influenced by distances far from zero, we truncate the distances beyond a given distance $w$ (known as the *truncation distance*). Including these larger distances in our analysis does not demonstrably improve the precision of abundance estimates [@Buckland:2001vm, sections 4.3 and 5.3].

Models for the detection function are expected to have the following properties [@buckland2015distance, Chapter 5]:

* *Shoulder*: we expect observers to be able to see objects near them, not just those directly in front of them. For this reason, we expect the detection function to be flat near the line or point.
* *Non-increasing*: we do not think that observers should be more likely to see distant objects than those nearer the transect. If this occurs, it usually indicates an issue with field procedure (that the distribution of objects with respect to the line, $\pi(y)$ is not what we expect), so we do not want the detection function to model this.
* *Model robust*: models should be flexible enough to fit many different shapes.
* *Pooling robust*: many factors can affect the probability of detection and it is not possible to measure all of these. We would like models to produce unbiased results without inclusion of these factors.
* *Estimator efficiency*: we would like models to have low variances, given they satisfy the other properties above (which, if satisfied, would give low bias).

Given these criteria, we can formulate models for $g$.

## Formulations

There is a wide literature on possible formulations for the detection function [@Buckland:1992fa; @Eidous:2005bj; @Becker:2009cj; @Giammarino:2014eg; @Miller:2015hw; @Becker:2015fi]. \code{Distance} includes the most popular of these models. Here we detail the most popular detection function approach: "key function plus adjustments" (K+A).

### Key function plus adjustments (K+A)

Key function plus adjustment terms (or adjustment series) models are formulated by taking a "key" function and optionally adding "adjustments" to it to improve the fit [@Buckland:1992fa]. Mathematically we formulate this as:
$$
g(y; \boldsymbol{\theta}) = k(y; \boldsymbol{\theta}_\text{key})\left( 1+ \alpha_O(y; \boldsymbol{\theta}_\text{adjust})\right),
$$
where $k$ is the key function and $\alpha_O$ is sum series of functions (given in Table \ref{tab:keyadj}), described as an *adjustment of order $O$*. Subscripts on the parameter vector indicate those parameters belonging to each part of the model (i.e. $\boldsymbol{\theta} = (\boldsymbol{\theta}_\text{key}, \boldsymbol{\theta}_\text{adjust})$).

Available models for the key are as follows:
$$
k(y) = \left\{
\begin{array}{l l}
  \exp\left(-\frac{y^2}{2 \sigma^2}\right) & \quad \text{half-normal,} \\
  1-\exp\left(\left(-\frac{y}{\sigma}\right)^{-b}\right) & \quad \text{hazard-rate,} \\
  1/w & \quad \text{uniform.}
\end{array} \right.
$$
Possible modelling options for key and adjustments are given in Table \ref{tab:keyadj} and illustrated in Figure \ref{fig:hnhr} and \ref{fig:keyadj}. We select the number of adjustment terms ($K$) by AIC (further details in "Model checking and model selection").

```{r hn-hr-par-comp, fig.width=10, fig.height=5, fig.cap="Half-normal (top row) and hazard-rate (bottom row) detection functions without adjustments, varying scale ($\\sigma$) and (for hazard-rate) shape ($b$) parameters (values are given above the plots). On the top row from left to right, the study species becomes more detectable (higher probability of detection at larger distances). The bottom row shows the hazard-rate model's more pronounced shoulder.\\label{fig:hnhr}", echo=FALSE}
par(mfrow=c(2,4), mar=c(3.5, 3, 2, 1) + 0.1)

## half-normal
g.hn <- function(x,sigma) exp(-x^2/(2*sigma^2))
for(this.sig in c(0.05, 0.25, 1, 10)){
  this.g <- function(x) g.hn(x, sigma=this.sig)
  curve(this.g, from=0, to=1, xlab="", ylab="",
        main=bquote(sigma == .(this.sig)),
        xlim=c(0,1), ylim=c(0,1), asp=1)
  title(xlab="Distance", line=2)
  title(ylab="Detection probability", line=2)
}

## hazard-rate
g.hr <- function(x, sigma, b) 1 - exp(-(x/sigma)^-b)
for(this.sig in c(0.1, 0.5)){
  for(this.b in c(5, 1)){
    this.g <- function(x) g.hr(x, sigma=this.sig, b=this.b)
    curve(this.g, from=0, to=1, xlab="", ylab="",
          main=bquote(sigma == .(this.sig) ~ .(", b") == .(this.b)),
          xlim=c(0,1), ylim=c(0,1), asp=1)
    title(xlab="Distance", line=2)
    title(ylab="Detection probability", line=2)
  }
}
```


\begin{table}
\caption{Modelling options for key plus adjustment series models for the detection function. Adapted from Buckland et al. (2001), section 2.4.}
\begin{tabular}{llll}
\hline
Key function   & Form   & Adjustment series & Form\\
\hline
 Uniform  & $1/w$   & cosine  & $\sum_{o=1}^O a_o \cos(o \pi y/w)$ \\
 & & Simple polynomial & $\sum_{o=1}^O a_o (y/w)^{2o}$ \\
 Half-normal  & $\exp\left(-\frac{y^2}{2 \sigma^2}\right)$ & cosine  & $\sum_{o=2}^O a_o \cos(o \pi y/w)$ \\
 & & Hermite polynomial & $\sum_{o=2}^O a_o H_{2o}(y/\sigma)$ \\
 Hazard-rate  & $1-\exp\left[-\left(\frac{y}{\sigma}\right)^{-b}\right]$ & cosine  & $\sum_{o=2}^O a_o \cos(o \pi y/w)$ \\
 & & Simple polynomial & $\sum_{o=2}^O a_o (y/w)^{2o}$ \\
\hline
\end{tabular}
\label{tab:keyadj}
\end{table}

When adjustment terms are used it may be necessary to standardise the results to ensure that $g(0)=1$, so we can redefine the detection function as:
$$
g(y; \boldsymbol{\theta}) = \frac{k(y; \boldsymbol{\theta}_\text{key})\left( 1+ \alpha_O(y; \boldsymbol{\theta}_\text{adjust})\right)}{k(0; \boldsymbol{\theta}_\text{key})\left( 1+ \alpha_O(0; \boldsymbol{\theta}_\text{adjust})\right)}.
$$

A disadvantage of K+A models is that we must resort to constrained optimisation [via the \pkg{Rsolnp} package; @rsolnp-pkg] to ensure that the resulting detection function is monotonic non-increasing over its range.

We do not always include adjustments (except in the case of the uniform key), in which case we refer to "key only" models (see the next section and "Model checking and model selection").

```{r adjust-mix-comp, fig.width=10, fig.height=3, fig.cap="Possible (though not necessarily plausible) shapes for the detection function when adjustments are included for half-normal and hazard-rate models.\\label{fig:keyadj}", echo=FALSE}
par(mfrow=c(1,4), mar=c(3.5, 3, 3, 1) + 0.1)

## half-normal with cosine
g <- function(x) exp(-x^2/(2*0.01^2))*(1+0.5*cos((2*pi*x)/0.025))
f <- function(x) g(x)/g(0)
curve(f, from=0, to=0.025, xlab="", ylab="", main="Half-normal with \n1 cosine adjustment", xlim=c(0,0.025), ylim=c(0,1))
title(xlab="Distance", line=2)
title(ylab="Detection probability", line=2)
# 2 cosines
g <- function(x) exp(-x^2/(2*0.01^2))*(1-0.06*cos((2*pi*x)/0.025)+0.25*cos((3*pi*x)/0.025))
f <- function(x) g(x)/g(0)
curve(f, from=0, to=0.025, xlab="", ylab="", main="Half-normal with \n2 cosine adjustments",xlim=c(0,0.025), ylim=c(0,1))
title(xlab="Distance", line=2)
title(ylab="Detection probability", line=2)

## hazard-rate with cosine
g <- function(x) (1 - exp(-(x/0.005)^-1.1))*(1-0.05*cos((2*pi*x)/0.025))
f <- function(x) g(x)/g(0)
curve(f, from=0, to=0.025, xlab="", ylab="", main="Hazard-rate with \n1 cosine adjustment", xlim=c(0,0.025), ylim=c(0,1))
title(xlab="Distance", line=2)
title(ylab="Detection probability", line=2)
# 2 cosines
g <- function(x) (1 - exp(-(x/0.005)^-1.9))*
                  (1+0.05*cos((2*pi*x)/0.025) + 0.25*cos((3*pi*x)/0.025))
f <- function(x) g(x)/g(0)
curve(f, from=0, to=0.025, xlab="", ylab="", main="Hazard-rate with \n2 cosine adjustments", xlim=c(0,0.025), ylim=c(0,1))
title(xlab="Distance", line=2)
title(ylab="Detection probability", line=2)
```


### Covariates

There are many factors that can affect the probability of detecting an object. These include things like the observer, size of group (if objects occur in groups), the vessel or platform used, the sea state, weather conditions and time of day. We assume that these variables affect detection only via the scale of the detection function (and do not affect the shape).

Covariates can be included in this formulation by considering the scale parameter from the half-normal or hazard-rate detection functions as a linear model (on the exponential scale) of the ($J$) covariates ($\mathbf{z}$; a vector of length $J$ for each observation):
$$
\sigma(\mathbf{z}) = \exp(\beta_0 + \sum_{j=1}^J \beta_j z_j).
$$

Including covariates has an important implication for our calculation of detectability. Because we do not know  the true distribution of the covariates,we must calculate the probability of detection conditional on the observed values of the covariates:
$$
p(\mathbf{z_i}) = \int_0^w \pi(y) g(y, \mathbf{z_i}; \boldsymbol{\theta}) dy,
$$
where $\mathbf{z_i}$ is the vector of $J$ covariates associated with observation $i$. For covariate models, we are calculating a value of "average" probability of detection (average in the sense of distance being integrated out) per observation. There will be as many unique values of $p(\mathbf{z_i})$ as there are unique covariate combinations in our data.

K+A models that include covariates and one or more adjustments cannot be guaranteed to be monotonic non-increasing for all covariate combinations, as we do not have any model for the distribution of the covariates. Without a distribution for the covariates, it's not possible to know what the behaviour of the detection function will be, so we cannot setup meaningful constraints -- so we cannot constrain the detection function to be monotonic. For this reason, we advise against using both adjustments and covariates in a detection function [see @Miller:2015hw for an example of when this can be problematic].


## Fitting detection functions in R

The workhorse of detection function fitting in \pkg{Distance} is the \code{ds} function. Here we show some of the possible formulations for the detection function we have seen above applied to the minke whale and amakihi data.

### Minke whale

We can fit a model to the minke whale data, setting the truncation at 1.5km and using the default options in \code{ds} very simply:
```{r minke-hn}
minke_hn <- ds(minke, truncation=1.5)
```
Note that \code{ds} will automatically select adjustment terms by AIC and show the selection steps.

Figure \ref{fig:minkeamakihi} (left panel) shows the result of calling \code{plot} on the resulting model object. We can also call \code{summary} on the model object to get summary information about the fitted model (we postpone this to the next section).

We can specify the form of the detection function via the \code{key=} argument to \code{ds}. For example, a hazard rate model can be fitted as:
```{r minke-hr-cos}
minke_hrcos <- ds(minke, truncation=1.5, key="hr")
```
\code{ds} fits a model with cosine adjustments (the default) but finds the AIC improvement to be insufficient to select the adjustment. 

### Amakihi

By default \code{ds} assumes the data given to it has been collected as line transects, but we can switch to point transects using \code{transect="point"}. We can include covariates in the scale parameter via the \code{formula=~...} argument to \code{ds}. A hazard-rate model for the amakihi that includes observer as a covariate can be specified by [truncating sightings at 82.5m, given in @Marques:2007ey]:
```{r amakihi-hr-obs}
amakihi_hr_obs <- ds(amakihi, truncation=82.5, transect="point",
                     key="hr", formula=~obs)
```
Note that here \code{ds} warns us that we have not supplied enough information to estimate abundance, unlike with the minke whale data.

As with the minke whale model, we can plot the resulting model (Figure \ref{fig:minkeamakihi}, middle panel). Rather than detection functions, Figure \ref{fig:minkeamakihi} shows the probability density functions for the amakihi models. Probability density function plots give a better sense of fit of the model than the detection functions, as for point data we must rescale the histogram when plotting the detection function to take into account the geometry of the point sampler. The amakihi model included covariates, so the plot shows the detection function averaged over levels/values of the covariate. Points on the plot indicate probability of detection for each observation. For the `amakihi_hr_obs` model we see fairly clear levels of the observer covariate in the points. Looking at the right panel of Figure \ref{fig:minkeamakihi}, we can see this is less clear when adding minutes after sunrise as a covariate into the model:
```{r amakihi-hr-obs-mas, warning=FALSE}
amakihi_hr_obs_mas <- ds(amakihi, truncation=82.5, transect="point",
                         key="hr", formula=~obs+mas)
```



```{r minke-amakihi-hn-plot, echo=FALSE, warning=FALSE, fig.width=9, fig.height=4, fig.cap="Left: fitted detection function overlayed on the histogram of observed distances for the minke whale data using half-normal model. Centre and right: plots of the probability density function for the amakihi models. Centre, hazard-rate with observer as a covariate; right, hazard-rate model with observer and minutes after sunrise as covariates. Points indicate probability of detection for a given observation (given that observations covariate values) and lines indicate the average detection function.\\label{fig:minkeamakihi}"}
par(mfrow=c(1, 3))
plot(minke_hn, showpoints=FALSE)
plot(amakihi_hr_obs, pdf=TRUE, cex=0.7)
plot(amakihi_hr_obs_mas, pdf=TRUE, cex=0.7)
```

# Model checking and model selection

As with models fitted using \code{lm} or \code{glm} in \proglang{R}, we can use \code{summary} to give useful information about our fitted model. For our hazard-rate model for the amakihi, with observer as a covariate:
```{r amakihi-summary}
summary(amakihi_hr_obs)
```
This summary information includes details of the data and model specification, as well as the values of the coefficients ($\beta_j$) and their uncertainties, an "average" value for the detectability (see "Estimating abundance and variance" for details on how this is calculated) and its uncertainty. The final line gives an estimate of abundance for the area covered by the survey (see the next section).


## Goodness of fit

To judge goodness of fit for detection functions when exact distances are used, we want to compare the cumulative distribution function (CDF) and empirical distribution function (EDF) for the detection function via a quantile-quantile plot (Q-Q plot). The CDF evaluates the probability of observing an object at a distance less than or equal to some value. The EDF gives the proportion of observations for which the CDF is less than or equal to that of a given distance. This can be interpreted as assessing whether the number of observations up to a given distance is in line with what the model says they should be (where the "given values" are the observed distances). As usual for Q-Q plots, "good" models will have values close to the line $y=x$, poor models will show greater deviations from that line.

We can inspect Q-Q plots visually, though this is prone to subjective judgments. Instead we can quantify the Q-Q plot's information using a Kolmogorov-Smirnov or Cramer-von Mises test [@Burnham:2004vd]. Both test whether points from the EDF and CDF are from the same distribution. The Kolmogorov-Smirnov uses the test statistic of the largest difference between a point on the Q-Q plot and the line $y=x$, whereas the Cramer-von Mises test uses the sum of all the differences. As it takes into account more information and is therefore more powerful, the Cramer-von Mises is generally preferred. A significant result from either test indicates that the EDF and CDF do not come from the same distribution (and therefore the model is does not fit the data well).

We can generate a Q-Q plot and test results using the \code{gof_ds} function. Figure \ref{amakihi-qq} shows the goodness of fit tests for two models for the amakihi data. We first fit a half-normal model without covariates or adjustments (setting \code{adjustment=NULL} will force \code{ds} to fit a model with no adjustments):
```{r amakihi-gof, fig.keep="none"}
amakihi_hn <- ds(amakihi, truncation=82.5, transect="point", key="hn", adjustment=NULL)
gof_ds(amakihi_hn)
gof_ds(amakihi_hr_obs_mas)
```
So we can therefore conclude that the half-normal model does not pass our goodness of fit tests and should be discarded. The corresponding Q-Q plots are shown in Figure \ref{amakihi-qq}, comparing the half-normal model with the hazard-rate model with observer and minutes after sunrise included.

```{r, amakihi-qq-comp, fig.width=6, fig.height=3, fig.cap="Comparison of quantile-quantile plots for a half-normal model (no adjustments, no covariates; left) and hazard-rate model with observer and minutes after sunrise (right) for the amakihi data.\\label{amakihi-qq}", echo=FALSE, results="hide"}
par(mfrow=c(1,2))
gof_ds(amakihi_hn, asp=1, cex=0.3)  # shrinking dot size to obliterate 45-deg line less
gof_ds(amakihi_hr_obs_mas, asp=1, cex=0.3)
```

## Model selection

Once we have a set of models which fit well, we can use Akaike's Information Criterion (AIC) to select between models. \pkg{Distance} includes a function to create table of summary information for fitted models, making it easy to get an overview of a large number of models. The \code{summarize_ds_models} function takes models as input and can be especially useful when paired with \pkg{knitr}'s \code{kable} function to create summary tables for publication [@knitr-pkg]. An example of this output is shown in Table \ref{tab:amakihi} and was generated by the following call to \code{summarize_ds_models}:
```{r summary-table-null, eval=FALSE}
summarize_ds_models(amakihi_hn, amakihi_hr_obs, amakihi_hr_obs_mas)
```

```{r summary-table, results="asis", echo=FALSE}
library("knitr")
kable(summarize_ds_models(amakihi_hn, amakihi_hr_obs, amakihi_hr_obs_mas),
      digits = 3, format = "latex", row.names = FALSE, escape=FALSE,
      booktabs=TRUE,
      caption="Summary for the detection function models fitted to the amakihi data. ``C-vM'' stands for Cramer-von Mises, $P_a$ indicates average detectability (see ``Estimating abundance and variance''), se indicates standard error. Models are sorted according to AIC.\\label{tab:amakihi}")
```


# Estimating abundance and variance

Though fitting the detection function is the primary modelling step in distance sampling, we are really interested in estimating detectability, and from that abundance. We also wish to calculate our uncertainty for each abundance estimate. This section addresses these issues mathematically before showing how to estimate abundance and its variance in \proglang{R}.

## Abundance

We wish to obtain the abundance in a study region, of which we have sampled a (representative) subset. To do this we first calculate the abundance in the area we have surveyed (the *covered area*) to obtain $\hat{N}_\text{C}$, we can then scale this up (based on the survey design) to the full study area by multiplying it by the ratio of covered area to study area. We discuss other methods for spatially explicit abundance estimation in "Extensions".

First, to estimate abundance in the covered area ($\hat{N}_\text{C}$), we use the estimates of detection probability (the $\{\hat{p}(\mathbf{z}_i); i=1,\ldots,n\}$, above) in a Horvitz-Thompson-like estimator:
\begin{equation}
\hat{N}_\text{C} = \sum_{i=1}^n\frac{s_i}{\hat{p}(\mathbf{z}_i)},
\label{ht}
\end{equation}
where $s_i$ are the sizes of the observed groups of objects, which is equal to 1 if objects only occur singly [@Borchers:2004wr]. @thompson2012sampling is the canonical reference to this type of estimator. Intuitively, we can think of the estimates of detectability ($\hat{p}(\mathbf{z}_i)$) as "inflating" the group sizes ($s_i$), we then sum over the detections ($i$) to obtain the abundance estimate. For models that do not include covariates, $\hat{p}(\mathbf{z}_i)$ is equal for all $i$, so this is equivalent to summing the groups and inflating that sum by dividing through by the corresponding $\hat{p} (=\hat{p}(\mathbf{z}_i) \forall i)$.

Having obtained the abundance in the covered area, we can then scale-up to the study area:
$$
\hat{N} = \frac{A}{a} \hat{N}_\text{C},
$$
where $A$ is the area of the study region to which to extrapolate the abundance estimate and $a$ is the covered area. For line transects $a=2wL$ (twice the truncation distance multiplied by the total length of transects surveyed, $L$) and for points $a=\pi w^2 T$ (where $\pi w^2$ is the area of a single surveyed circle and $T$ is the sum of the number of visits to the sampled points).

We can use the Horvitz-Thompson-like estimator to calculate the "average" detectability for models which include covariates. We can consider what single detectability value would give the estimated $\hat{N}_\text{C}$ and therefore calculate:
$$
\hat{P_a} = n/\hat{N}_\text{C}.
$$
This can be a useful summary statistic, giving us an idea of how detectable our $n$ observed animals would have to be to estimate the same $\hat{N}$ if there were no observed covariates. It can also be compared to similar estimates in mark-recapture studies. $P_a$ is included in the \code{summary} output and the table produced by \code{summarize_ds_models}. 

### Stratification

We may wish to calculate abundance estimates for some sub-regions of the study region, we call these areas *strata*. For example, strata may be defined by habitat types or animal gender (or some combination) which may be of interest for biological or management reasons. To calculate estimates for a given stratification each observation must occur in a stratum which must be labelled with a \code{Region.Label} and have a corresponding \code{Area} (if we are using an animal characteristic like gender, we would have the areas be the same but if we were using say forested vs. wetland habitat the areas of those strata would be different). Finally, we must also know the stratum in which each observation occurs.

As an example the minke whale data consists of two strata: \code{North} and \code{South} relating to a stratum further away and nearer the Antarctic ice edge, respectively. Figure \ref{fig:minke-strata} shows the two strata, along with observations and transect lines.


## Variance

Here we take an intuitive approach to uncertainty estimation, for a full derivations consult @Marques:2003vb. Uncertainty in $\hat{N}$ comes from two sources:

1. *Model parameter uncertainty*, from the estimation of the detection function parameters $\boldsymbol{\theta}$.
2. *Sampling uncertainty*, from the distribution of objects along the transect lines or between visiting occasions for points.

We can see this by looking at the Horvitz-Thompson-like estimation in (\ref{ht}) and consider the terms which are random. These are: the detectability $\hat{p}(\mathbf{z}_i)$ (and hence the parameters of the detection function it is derived from) and $n$, the number of observations. We assume that the observed group size ($s_i$) is recorded without error.

Model parameter uncertainty can be addressed using standard maximum likelihood theory. We can invert the Hessian matrix of the likelihood to obtain a variance-covariance matrix. We can then pre- and post-multiply this by the derivatives of $\hat{N}_\text{C}$ with respect to the parameters of the detection function
$$
\widehat{\text{Var}}_\text{model}\left( \hat{N}_\text{C}\right) = \left(\frac{\partial \hat{N}_\text{C}}{\partial\hat{\boldsymbol{\theta}}}\right)^\text{T} \left(\hat{\mathbf{H}}(\hat{\boldsymbol{\theta}})^{-1} \right)\frac{\partial \hat{N}_\text{C}}{\partial\hat{\boldsymbol{\theta}}}
$$
where the partial derivatives of $\hat{N}_\text{C}$ are evaluated at the MLE ($\hat{\theta}$) and $\mathbf{H}$ is the first partial Hessian (outer product of first derivatives of the log likelihood) for numerical stability [@Buckland:2001vm, p 62]. Note that although we calculate uncertainty in $\hat{N}_\text{C}$, we can trivially scale-up to variance of $\hat{N}$ (by noting that $\hat{N} = \frac{A}{a} \hat{N}_\text{C}$ and therefore $\widehat{\text{Var}}_\text{model}\left(\hat{N}\right) = \left( \frac{A}{a} \right)^2 \widehat{\text{Var}}_\text{model} \left(\hat{N}_\text{C} \right)$).

Sampling uncertainty can be characterised by the *encounter rate*: the number of objects per unit transect (rather than just $n$). When covariates are not included in the detection function we can define the encounter rate as $n/L$ for line transects (where $L$ is the total line length) or $n/T$ for point transects (where $T$ is the total number of visits summed over all points). When covariates are included in the detection function, it is recommended that we substitute the $n$ in the encounter rate with the estimated abundance $\hat{N}_\text{C}$ as this will take into account the effects of the covariates [@Innes:2002ka].

For line transects, by default, \pkg{Distance} uses a variation of the estimator "R2" from @Fewster:2009ku replacing number of observations per sample with the estimated abundance per sample [@Innes:2002ka; @Marques:2003vb]:
$$
\widehat{\text{Var}}_{\text{encounter},R2}\left( \hat{N}_\text{C}\right) = \frac{K}{L^2(K-1)} \sum_{k=1}^{K} l_k^2 \left( \frac{\hat{N_{\text{C}, k}}}{l_k} - \frac{\hat{N_\text{C}}}{L}\right)^2,
$$
where $l_k$ are the lengths of the $K$ transects (such that $L = \sum_{k=1}^K l_k$) and $\hat{N_{C,k}}$ is the abundance in the covered area for transect $k$. Whereas for points we use estimator "P3" from @Fewster:2009ku but again replacing $n$ by $\hat{N}_\text{C}$ in the encounter rate definition, we obtain the following estimator:
$$
\widehat{\text{Var}}_{\text{encounter},P3}\left( \hat{N}_\text{C}\right) = \frac{1}{T(K-1)} \sum_{k=1}^{K} t_k \left( \frac{\hat{N_{\text{C}, k}}}{t_k} - \frac{\hat{N_\text{C}}}{T}\right)^2,
$$
where $t_k$ is the number of visits to point $k$ and $T = \sum_{k=1}^K t_k$ (the total number of visits to all points is the sum of the visits to each point). Again, it is straightforward to calculate the encounter rate variance for $\hat{N}$ from the encounter rate variance for $\hat{N}_\text{C}$.

Other formulations for the encounter rate variance are discussed in detail in @Fewster:2009ku. \pkg{Distance} implements all of the estimators of encounter rate variance given in that article. The \code{varn} manual page gives further advice and technical detail on encounter rate variance.

We combine these two sources of variance by noting that squared coefficients of variation (approximately) add [@Goodman:1960dd] (sometimes referred to as "the delta method").


## Estimating abundance and variance in R

Returning to the minke whale data, we have the necessary information to calculate $A$ and $a$ above, so we can estimate abundance and its variance. When we supply data to \code{ds} in the "flatfile" format given above, \code{ds} will automatically calculate abundance estimates based on the survey information in the data.

Having already fitted a model to the minke whale data, we can see the results of the abundance estimation by viewing the model summary:
```{r minke-N-summary}
summary(minke_hn)
```
This prints a rather large amount of information: first the detection function summary, then three tables:

1. \code{Summary statistics}: giving the areas, covered areas, effort, number of observations, number of transects, encounter rate, its standard error and coefficient of variation for each stratum, then a total for the whole study area.
2. \code{Abundance}: giving estimates, standard errors, coefficients of variation, lower and upper confidence intervals and finally the degrees of freedom for each stratum's abundance estimate, then a total for the whole study area.
3. \code{Density}: lists the same statistics as \code{Abundance} but for a density estimate.

The summary can be more concisely expressed by extracting information from the summary object. This object is a \code{list} of \code{data.frames}, so we can again use the \code{kable} function from \pkg{knitr} to create summary tables of abundance estimates and measures of precision, such as Table \ref{minke-abund}. We prepare the \code{data.frame} as follows before using \code{kable}:

```{r minke-abundance-table-code, eval=FALSE}
minke_table <- summary(minke_hn)$dht$individuals$N
minke_table$lcl <- minke_table$ucl <- minke_table$df <- NULL
colnames(minke_table) <- c("Stratum", "$\\hat{N}$", "$\\text{se}(\\hat{N}$)", 
                           "$\\text{CV}(\\hat{N}$)")
```

```{r minke-abundance-table, echo=FALSE}
minke_table <- summary(minke_hn)$dht$individuals$N
minke_table$lcl <- minke_table$ucl <- minke_table$df <- NULL
colnames(minke_table) <- c("Stratum", "$\\hat{N}$", "$\\text{se}(\\hat{N}$)", 
                           "$\\text{CV}(\\hat{N}$)")
kable(minke_table, digits=3, format="latex", booktabs=TRUE,
      row.names=FALSE, escape=FALSE,caption="Summary of abundance estimation for the half-normal model for the minke whale data.\\label{minke-abund}")
```


# Extensions

The features of \pkg{Distance} are deliberately limited to provide a simplified interface for users. For more complex analyses of distance sampling data, there are further related packages for modelling in \proglang{R}.

We noted at the start of the article that \pkg{Distance} is a simpler wrapper around the package \pkg{mrds}. Additional features are available in \pkg{mrds} including the ability to model data where the assumption that detection is certain at zero distance from the line or point is violated, using mark-recapture type methods for double observer surveys [see @Burt:2014gu for an introduction].

The abundance estimates calculated here are based on the assumption that within a given stratum density is uniform. We may extend this approach to many strata, making the area of each very small to account for small-scale variation in space. A more rigorous approach is to build a spatial model incorporating spatially-referenced environmental data (for example derived from GIS products). \pkg{Distance} interfaces with one such package to perform this type of analysis: \pkg{dsm} [@dsm-pkg]. So-called "density surface modelling" uses the generalized additive model framework [e.g. @Wood:2006vg] to build models of abundance (adjusting counts for imperfect detectability) as a function of environmental covariates, as part of a two stage model [@Hedley:2004et; @Miller:2013fq].

Further distance sampling complexities such as uncertainty in measured covariates (e.g. cluster size) and model uncertainty (when two models have similar fit but substantially different estimates) can be incorporated using the multi-analysis distance sampling package \pkg{mads} [@mads-pkg]. In addition, \pkg{mads} can also incorporate sightings for which species identification could not be achieved. This is done by estimating the abundance of these 'unidentified' sightings and pro-rating them as described in [@Gerrodette:2005].

In addition to the analyses, an equally if not more important stage of a distance sampling survey is the survey design. We have therefore developed a package which allows users to test out different designs in their specific study region and tailor population attributes to reflect the species they are working with. In this way \pkg{DSsim} [@DSsim-pkg] allows users to more easily identify challenges unique to their study and select a survey design which is likely to yield the most accurate and precise results. 

Distance for Windows has many users (over 45,000 downloads since 2002) and they may be overwhelmed by the prospect of switching existing analyses to \proglang{R}. For that reason we have created the \pkg{readdst} [@readdst-pkg] package to interface with projects created by Distance for Windows. The package can take analyses created using the CDS, MCDS and MRDS engines in Distance for Windows and extract data and create equivalent models in \proglang{R}. \pkg{readdst} can also run these analyses and test the resulting statistics (for example, $\hat{N}$ or $\hat{P_a}$) calculated in \proglang{R} against those calculated by Distance for Windows. We hope that \pkg{readdst} will provide a useful transition to \proglang{R} for interested users. \pkg{readdst} is currently available on GitHub at \url{https://github.com/distancedevelopment/readdst}.

# Conclusion

We have given an introduction as to how to perform a distance sampling analysis in \proglang{R}. We have covered the possible models for detectability, model checking and selection and finally abundance and variance estimation.

In combination with tools such as \pkg{knitr} and \pkg{rmarkdown} [@rmarkdown-pkg], the helper functions in \pkg{Distance} provide a useful set of tools to perform reproducible analyses of wildlife abundance for both managers and ecologists. We hope that this paper provides useful examples for those wishing to pursue this. More information on distance sampling can be found at \url{http://distancesampling.org} and a mailing list is maintained at \url{https://groups.google.com/forum/#!forum/distance-sampling}.

We note that there are other packages available for performing distance sampling analyses in \proglang{R} but believe that \pkg{Distance} is the most flexible and feature-complete. Appendix A gives a feature comparison between \pkg{Distance} and other \proglang{R} packages for analysis of distance sampling data.


# Acknowledgements

The authors would like to thank the many users of \pkg{Distance}, \pkg{mrds} and DISTANCE who have contributed bug reports and suggestions for improvements over the years. We would particularly like to thank Steve Buckland, David Borchers, Tiago Marques, Jon Bishop and Lorenzo Milazzo for their contributions.


# Bibliography
